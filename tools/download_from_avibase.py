#!/usr/bin/env python3
"""
ä»Avibaseè·å–é¸Ÿç±»å›¾ç‰‡
"""

import urllib.request
import urllib.parse
import re
import ssl
import os
import sys
import json
from html.parser import HTMLParser

# åˆ›å»ºSSL contextï¼Œè·³è¿‡è¯ä¹¦éªŒè¯
ssl_context = ssl.create_default_context()
ssl_context.check_hostname = False
ssl_context.verify_mode = ssl.CERT_NONE

# å•æ¬¡æŠ“å–å¹¶å¤ç”¨çš„ CN æ¸…å• HTML ç¼“å­˜
_CN_CHECKLIST_HTML_CACHE = None

def fetch_cn_checklist_html(force: bool = False) -> str:
    """è·å–ä¸­å›½åå½•é¡µé¢HTMLï¼Œæ”¯æŒï¼š
    - å•æ¬¡æŠ“å–å¹¶ç¼“å­˜ï¼ˆåŒä¸€è¿›ç¨‹å†…å¤šæ¬¡æŸ¥è¯¢å…±ç”¨ï¼‰
    - ç¯å¢ƒå˜é‡ AVIBASE_CN_HTML æŒ‡å®šæœ¬åœ°HTMLæ–‡ä»¶ï¼ˆä¾¿äºæ‰¹é‡è§£æä¸æµ‹è¯•ï¼‰
    """
    global _CN_CHECKLIST_HTML_CACHE
    if not force and _CN_CHECKLIST_HTML_CACHE is not None:
        return _CN_CHECKLIST_HTML_CACHE  # type: ignore

    local_path = os.environ.get('AVIBASE_CN_HTML', '').strip()
    if local_path:
        try:
            with open(local_path, 'r', encoding='utf-8', errors='ignore') as f:
                _CN_CHECKLIST_HTML_CACHE = f.read()
            print(f"   CNæ¸…å•: æœ¬åœ°æ–‡ä»¶ {local_path}")
            return _CN_CHECKLIST_HTML_CACHE  # type: ignore
        except Exception as e:
            print(f"   âš ï¸  è¯»å–æœ¬åœ°CNæ¸…å•å¤±è´¥: {e}ï¼Œæ”¹ä¸ºåœ¨çº¿æŠ“å–")

    url = "https://avibase.bsc-eoc.org/checklist.jsp?region=CN&list=clements_2024&lang=EN"
    print(f"   CNæ¸…å•: {url}")
    req = urllib.request.Request(url, headers={
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X) AppleWebKit/537.36 (KHTML, like Gecko) Safari/537.36'
    })
    with urllib.request.urlopen(req, context=ssl_context) as resp:
        _CN_CHECKLIST_HTML_CACHE = resp.read().decode('utf-8', 'ignore')
    return _CN_CHECKLIST_HTML_CACHE  # type: ignore

def _parse_cn_rows(html: str):
    """è§£æCNæ¸…å•HTMLï¼Œè¿”å›[(en, sci, cn, avibaseid), ...]ï¼Œ
    ç»„åˆ HTMLParserï¼ˆä¸»ï¼‰ + æ­£åˆ™å…œåº•ï¼Œå¹¶åšå»é‡ã€‚
    """
    class CNTableParser(HTMLParser):
        def __init__(self):
            super().__init__()
            self.in_tr = False
            self.in_td = False
            self.td_index = -1
            self.in_i = False
            self.cur_en = []
            self.cur_sci = []
            self.cur_cn = []
            self.cur_id = ''
            self.rows = []

        def handle_starttag(self, tag, attrs):
            tag = tag.lower()
            if tag == 'tr':
                self.in_tr = True
                self.in_td = False
                self.td_index = -1
                self.in_i = False
                self.cur_en, self.cur_sci, self.cur_cn = [], [], []
                self.cur_id = ''
            elif self.in_tr and tag == 'td':
                self.in_td = True
                self.td_index += 1
            elif self.in_tr and tag == 'a':
                href = dict(attrs).get('href', '')
                m = re.search(r"species\.jsp\?avibaseid=([A-F0-9]{16})", href, re.IGNORECASE)
                if m:
                    self.cur_id = m.group(1)
            elif self.in_tr and tag == 'i':
                self.in_i = True

        def handle_endtag(self, tag):
            tag = tag.lower()
            if tag == 'td':
                self.in_td = False
                self.in_i = False
            elif tag == 'i':
                self.in_i = False
            elif tag == 'tr' and self.in_tr:
                en = ' '.join(''.join(self.cur_en).split())
                sci = ' '.join(''.join(self.cur_sci).split())
                cn = ' '.join(''.join(self.cur_cn).split())
                if self.cur_id:
                    self.rows.append((en, sci, cn, self.cur_id))
                self.in_tr = False

        def handle_data(self, data):
            if not (self.in_tr and self.in_td):
                return
            if self.td_index == 0:
                self.cur_en.append(data)
            elif self.td_index == 1 and self.in_i:
                self.cur_sci.append(data)
            elif self.td_index >= 2:
                self.cur_cn.append(data)

    parser = CNTableParser()
    parser.feed(html)
    rows = list(parser.rows)

    # å…œåº•ï¼šæ­£åˆ™ä»<tr>ç‰‡æ®µæŠ“å–
    def fallback_parse_rows(full_html: str):
        out = []
        pat = re.compile(
            r"<tr[^>]*>\s*<td>(?P<en>.*?)</td>\s*<td>\s*<a[^>]*href=\"species\\.jsp\?avibaseid=(?P<id>[A-F0-9]{16})\"[^>]*>\s*<i>(?P<sci>[^<]+)</i>\s*</a>\s*</td>\s*<td>(?P<cn>.*?)</td>\s*</tr>",
            re.IGNORECASE | re.DOTALL,
        )
        def strip_tags(s: str) -> str:
            s = re.sub(r"<[^>]+>", " ", s)
            s = " ".join(s.split())
            return s
        for m in pat.finditer(full_html):
            en = strip_tags(m.group('en'))
            sci = strip_tags(m.group('sci'))
            cn = strip_tags(m.group('cn'))
            aid = m.group('id')
            out.append((en, sci, cn, aid))
        return out

    fb_rows = fallback_parse_rows(html)
    seen, merged = set(), []
    for en, sci, cn, aid in rows + fb_rows:
        if aid not in seen:
            seen.add(aid)
            merged.append((en, sci, cn, aid))
    return merged

def search_cn_checklist(scientific_name: str, common_name: str):
    """ä»…ä»ä¸­å›½åå½•(checklist.jsp?region=CN)è§£æè¡¨æ ¼ï¼Œè¿”å›åŒ¹é…è¡Œçš„ avibaseid åˆ—è¡¨ï¼ˆæŒ‰å‡ºç°é¡ºåºï¼‰ã€‚
    åŒ¹é…ï¼šåŒä¸€ <tr> ä¸­çš„è‹±æ–‡å/å­¦å/ä¸­æ–‡åä»»æ„ä¸€ä¸ªä¸å…¥å‚åŒ¹é…ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼Œå…è®¸ç©ºæ ¼å·®å¼‚ï¼‰ã€‚
    æ”¯æŒå•æ¬¡æŠ“å–ç¼“å­˜ä¸æœ¬åœ°HTMLè¦†ç›–ï¼ˆAVIBASE_CN_HTMLï¼‰ã€‚
    """
    try:
        html = fetch_cn_checklist_html(force=False)
        rows = _parse_cn_rows(html)
        sci_norm = (scientific_name or '').strip().lower()
        en_norm = (common_name or '').strip().lower()
        ids = []
        for en, sci, cn, avb in rows:
            en_l = en.lower()
            sci_l = sci.lower()
            if (sci_norm and sci_l == sci_norm) or (en_norm and (en_norm == en_l or en_norm in en_l)):
                ids.append(avb)
        ids = list(dict.fromkeys(ids))
        if ids:
            print(f"   âœ… CNæ¸…å•å‘½ä¸­ {len(ids)} ä¸ªå€™é€‰: {', '.join(ids[:5])}{' ...' if len(ids)>5 else ''}")
        else:
            print("   âš ï¸  CNæ¸…å•æœªå‘½ä¸­")
        return ids
    except Exception as e:
        print(f"   âš ï¸  CNæ¸…å•è·å–å¤±è´¥: {e}")
        return []

def search_cn_checklist_multi(queries):
    """æ‰¹é‡æŸ¥è¯¢ï¼ˆå•æ¬¡æŠ“å–ï¼Œå¤ç”¨HTMLï¼‰ã€‚
    å‚æ•°ï¼š[(scientific_name, common_name), ...]
    è¿”å›ï¼š{ (scientific_name, common_name): [avibaseid, ...] }
    """
    html = fetch_cn_checklist_html(force=False)
    rows = _parse_cn_rows(html)
    result = {}
    for sci_in, en_in in queries:
        sci_norm = (sci_in or '').strip().lower()
        en_norm = (en_in or '').strip().lower()
        ids = []
        for en, sci, cn, avb in rows:
            en_l = en.lower()
            sci_l = sci.lower()
            if (sci_norm and sci_l == sci_norm) or (en_norm and (en_norm == en_l or en_norm in en_l)):
                ids.append(avb)
        result[(sci_in, en_in)] = list(dict.fromkeys(ids))
    print(f"   âœ… æ‰¹é‡æŸ¥è¯¢å®Œæˆï¼Œå…± {len(queries)} ä¸ªç‰©ç§")
    return result


def search_avibase(bird_name):
    """ä»…ä½¿ç”¨ä¸­å›½åå½•é¡µé¢ä½œä¸ºå€™é€‰æ¥æºï¼Œè¿”å› avibaseid åˆ—è¡¨ã€‚
    å…¼å®¹æ—§æ¥å£ï¼Œå†…éƒ¨èµ°å•æ¬¡æŠ“å–ç¼“å­˜ã€‚
    """
    candidates_overall = []
    try:
        ids = search_cn_checklist(scientific_name=bird_name, common_name=bird_name)
        candidates_overall.extend(ids)
    except Exception:
        pass
    if not candidates_overall:
        print(f"   âŒ æœªæ‰¾åˆ°ç‰©ç§")
        return []
    return candidates_overall

def get_flickr_photos(avibaseid):
    """è·å–ç‰©ç§çš„Flickrç…§ç‰‡URL"""
    # ä¿®æ”¹ä¸ºè®¿é—®Flickræ ‡ç­¾é¡µï¼Œè¿™é‡Œæœ‰æ›´å¤šç…§ç‰‡
    species_url = f"https://avibase.bsc-eoc.org/species.jsp?avibaseid={avibaseid}&lang=EN&sec=flickr"
    
    print(f"\nğŸ“¥ è·å–Flickrç…§ç‰‡...")
    print(f"   URL: {species_url}")
    
    try:
        with urllib.request.urlopen(species_url, context=ssl_context) as response:
            html = response.read().decode('utf-8')
            
            # æŸ¥æ‰¾Flickrå›¾ç‰‡URL
            # æ ¼å¼: https://live.staticflickr.com/XXXX/YYYYYY_ZZZZ_SIZE.jpg
            flickr_pattern = r'(https://live\.staticflickr\.com/[^"\']+_[a-z]\.jpg)'
            flickr_matches = re.findall(flickr_pattern, html)
            
            # ä¹ŸæŸ¥æ‰¾farmæ ¼å¼
            farm_pattern = r'(https://farm[0-9]+\.staticflickr\.com/[^"\']+_[a-z]\.jpg)'
            farm_matches = re.findall(farm_pattern, html)
            
            # åˆå¹¶å¹¶å»é‡
            all_photos = list(dict.fromkeys(flickr_matches + farm_matches))  # ä¿æŒé¡ºåºçš„å»é‡
            
            if all_photos:
                print(f"   âœ… æ‰¾åˆ° {len(all_photos)} å¼ ç…§ç‰‡")
                
                # è½¬æ¢ä¸ºå¤§å›¾URL
                large_photos = []
                for url in all_photos[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                    # å°†_n.jpg (small 320) æ›¿æ¢ä¸º _b.jpg (large 1024)
                    large_url = re.sub(r'_[a-z]\.jpg$', '_b.jpg', url)
                    large_photos.append(large_url)
                
                # æ˜¾ç¤ºå‰3å¼ çš„URL
                print(f"   å°†ä¸‹è½½å‰3å¼ :")
                for i, url in enumerate(large_photos[:3], 1):
                    print(f"      {i}. {url}")
                
                return large_photos
            else:
                print(f"   âš ï¸  æœªæ‰¾åˆ°ç…§ç‰‡")
                return []
    
    except Exception as e:
        print(f"   âŒ é”™è¯¯: {e}")
        return []

def download_image(url, output_path):
    """ä¸‹è½½å›¾ç‰‡"""
    try:
        print(f"\n   ğŸ“¥ ä¸‹è½½: {os.path.basename(output_path)}")
        
        with urllib.request.urlopen(url, context=ssl_context) as response:
            data = response.read()
            
            with open(output_path, 'wb') as f:
                f.write(data)
            
            size_kb = len(data) / 1024
            print(f"      âœ… æˆåŠŸ: {size_kb:.1f}KB")
            return True
            
    except Exception as e:
        print(f"      âŒ å¤±è´¥: {e}")
        return False

def download_avibase_photos(bird_name, scientific_name, output_dir, avibaseid_override=None, target_count: int = 3):
    """ä¸‹è½½Avibaseä¸Šçš„é¸Ÿç±»ç…§ç‰‡
    - è‹¥å•å¼ ä¸‹è½½å¤±è´¥ï¼Œå°†è‡ªåŠ¨è·³è¿‡å¹¶å°è¯•ä¸‹ä¸€å¼ ï¼Œç›´åˆ°å‡‘æ»¡ target_count æˆ–æ— æ›´å¤šå¯ç”¨é“¾æ¥ã€‚
    - ä¿å­˜ä¸‹è½½çš„ç…§ç‰‡å…ƒæ•°æ®ï¼ˆFlickr photo IDç­‰ï¼‰
    """
    
    print("="*60)
    print(f"ä»Avibaseä¸‹è½½: {bird_name}")
    print(f"å­¦å: {scientific_name}")
    print("="*60)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # åˆå§‹åŒ–å…ƒæ•°æ®åˆ—è¡¨
    metadata_list = []
    
    # 1. è·å– avibaseidï¼ˆä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„è¦†ç›–ï¼‰ï¼›å¦åˆ™æŒ‰â€œç¬¬ä¸€æ¡ä¼˜å…ˆï¼Œè‹¥æ— å›¾åˆ™è¯•ä¸‹ä¸€æ¡â€çš„é¡ºåº
    avibaseid = avibaseid_override
    if not avibaseid:
        candidates = search_avibase(scientific_name)
        if not candidates:
            print(f"\nâš ï¸  æœªæ‰¾åˆ°ç‰©ç§ä¿¡æ¯ï¼Œå°è¯•ä½¿ç”¨è‹±æ–‡åæœç´¢...")
            candidates = search_avibase(bird_name)
        preselected_photos = []
        for cid in candidates[:10]:  # æŒ‰å‡ºç°é¡ºåº
            photos = get_flickr_photos(cid)
            if photos:
                avibaseid = cid
                preselected_photos = photos
                break
    
    if not avibaseid:
        print(f"\nâŒ æ— æ³•æ‰¾åˆ°ç‰©ç§")
        return 0
    
    # 2. è·å–ç…§ç‰‡URLï¼ˆè‹¥ä¸Šä¸€æ­¥å·²æœ‰é¢„é€‰åˆ—è¡¨åˆ™å¤ç”¨ï¼‰
    try:
        photo_urls  # type: ignore
    except NameError:
        photo_urls = []
    if not photo_urls:
        try:
            preselected_photos  # type: ignore
        except NameError:
            preselected_photos = []
    photo_urls = preselected_photos or get_flickr_photos(avibaseid)
    
    if not photo_urls:
        print(f"\nâš ï¸  æ²¡æœ‰å¯ç”¨çš„ç…§ç‰‡")
        return 0
    
    # 3. ä¸‹è½½ç…§ç‰‡ï¼ˆè·å–å‰3å¼ ï¼‰
    print(f"\nğŸ“¥ å¼€å§‹ä¸‹è½½ç…§ç‰‡åˆ°: {output_dir}")
    print(f"   å…±æ‰¾åˆ° {len(photo_urls)} å¼ ï¼Œå°†å°è¯•ä¸‹è½½ {target_count} å¼ ")

    success_count = 0
    tried = 0
    seq = 1
    for url in photo_urls:
        if success_count >= target_count:
            break
        tried += 1
        filename = url.split('/')[-1]
        output_filename = f"avibase_{seq}_{filename}"
        output_path = os.path.join(output_dir, output_filename)
        
        # æå– Flickr photo IDï¼ˆä»æ–‡ä»¶åï¼‰
        # æ ¼å¼: PHOTOID_SECRET_SIZE.jpg
        flickr_id_match = re.match(r'(\d+)_[a-z0-9]+_[a-z]\.jpg', filename)
        flickr_id = flickr_id_match.group(1) if flickr_id_match else None
        
        if download_image(url, output_path):
            success_count += 1
            
            # ä¿å­˜å…ƒæ•°æ®
            photo_metadata = {
                'filename': output_filename,
                'flickr_photo_id': flickr_id,
                'flickr_url': f"https://www.flickr.com/photo.gne?id={flickr_id}" if flickr_id else None,
                'source_url': url,
                'note': 'ç½²åä¿¡æ¯éœ€ä»Flickrè·å–'
            }
            metadata_list.append(photo_metadata)
            
            seq += 1

    # ä¿å­˜å…ƒæ•°æ®åˆ°JSONæ–‡ä»¶
    if metadata_list:
        # æŸ¥æ‰¾çˆ¶ç›®å½•çš„ download_metadata.json
        parent_dir = os.path.dirname(output_dir)
        metadata_file = os.path.join(parent_dir, 'download_metadata.json')
        
        # è¯»å–ç°æœ‰å…ƒæ•°æ®æˆ–åˆ›å»ºæ–°çš„
        if os.path.exists(metadata_file):
            with open(metadata_file, 'r', encoding='utf-8') as f:
                all_metadata = json.load(f)
        else:
            all_metadata = {'macaulay': [], 'inaturalist': [], 'wikimedia': [], 'avibase': []}
        
        # æ›´æ–° avibase éƒ¨åˆ†
        all_metadata['avibase'] = metadata_list
        
        # ä¿å­˜
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(all_metadata, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“„ å…ƒæ•°æ®å·²ä¿å­˜åˆ°: {metadata_file}")

    print(f"\n[æ—¥å¿—] Avibase ä¸‹è½½ç»Ÿè®¡: å°è¯• {tried}ï¼ŒæˆåŠŸ {success_count}ï¼Œç›®æ ‡ {target_count}")
    print(f"\n{'='*60}")
    print(f"âœ… ä¸‹è½½å®Œæˆ: {success_count}/{target_count} å¼ ç…§ç‰‡")
    print(f"{'='*60}\n")
    
    return success_count

def parse_batch_file(batch_path: str):
    """è§£ææ‰¹é‡æ–‡ä»¶ã€‚æ”¯æŒCSVæˆ–TXTï¼š
    - é€—å·/åˆ¶è¡¨ç¬¦åˆ†éš”: common_name, scientific_name, output_dir
    - æˆ–ä¸‰åˆ—ç”¨ç©ºç™½åˆ†éš”ï¼ˆcommon_name å­¦å è¾“å‡ºç›®å½•ï¼‰
    - å¿½ç•¥ç©ºè¡Œä¸ä»¥#å¼€å¤´çš„æ³¨é‡Šè¡Œ
    è¿”å›: [(common, scientific, output_dir)]
    """
    rows = []
    with open(batch_path, 'r', encoding='utf-8', errors='ignore') as f:
        for line in f:
            line = line.strip()
            if not line or line.startswith('#'):
                continue
            parts = [p.strip() for p in re.split(r"[,\t]", line)]
            if len(parts) < 3:
                parts = line.split()
            if len(parts) < 3:
                print(f"   âš ï¸  è·³è¿‡æ— æ³•è§£æçš„è¡Œ: {line}")
                continue
            common, scientific, outdir = parts[0], parts[1], ' '.join(parts[2:])
            rows.append((common, scientific, outdir))
    return rows

def main():
    """ä¸»å‡½æ•°"""
    args = sys.argv[1:]
    if not args or ('--help' in args or '-h' in args):
        print("ç”¨æ³•:")
        print("  å•ä¸ªç‰©ç§: python3 download_from_avibase.py <è‹±æ–‡å> <å­¦å> <è¾“å‡ºç›®å½•> [avibaseid] [target_count]")
        print("  æ‰¹é‡æ¨¡å¼: python3 download_from_avibase.py --batch <æ–‡ä»¶è·¯å¾„> [target_count]")
        print("ç¤ºä¾‹:")
        print("  python3 download_from_avibase.py \"Oriental Greenfinch\" \"Chloris sinica\" \"images/goldfinch/avibase\"")
        print("  python3 download_from_avibase.py --batch birds.csv 3")
        sys.exit(0)

    target_count = 3
    try:
        if args[0] == '--batch':
            if len(args) < 2:
                print("âŒ ç¼ºå°‘æ‰¹é‡æ–‡ä»¶è·¯å¾„")
                sys.exit(1)
            batch_file = args[1]
            if len(args) >= 3 and args[2].isdigit():
                target_count = int(args[2])
            rows = parse_batch_file(batch_file)
            if not rows:
                print("âš ï¸  æ‰¹é‡æ–‡ä»¶ä¸ºç©ºæˆ–ä¸å¯è§£æ")
                sys.exit(1)
            # å•æ¬¡æŠ“å–å¹¶ç¼“å­˜CNæ¸…å•ï¼ˆè‹¥æœªè®¾ç½®ENVåˆ™è”ç½‘æŠ“å–ä¸€æ¬¡ï¼‰
            try:
                fetch_cn_checklist_html(force=False)
            except Exception as e:
                print(f"âš ï¸  é¢„æŠ“å–CNæ¸…å•å¤±è´¥: {e}")
            total_success = 0
            for common, scientific, outdir in rows:
                cnt = download_avibase_photos(common, scientific, outdir, avibaseid_override=None, target_count=target_count)
                total_success += cnt
            print(f"\nâœ… æ‰¹é‡å®Œæˆï¼Œåˆè®¡æˆåŠŸä¸‹è½½ {total_success} å¼ ")
            return
        else:
            # å•ç‰©ç§æ¨¡å¼
            if len(args) < 3:
                print("âŒ å‚æ•°ä¸è¶³ã€‚ç”¨æ³•: python3 download_from_avibase.py <è‹±æ–‡å> <å­¦å> <è¾“å‡ºç›®å½•> [avibaseid] [target_count]")
                sys.exit(1)
            bird_name = args[0]
            scientific_name = args[1]
            output_dir = args[2]
            avibaseid_override = args[3] if len(args) >= 4 and len(args[3]) == 16 else None
            if len(args) >= 4 and args[3].isdigit():
                target_count = int(args[3])
            if len(args) >= 5 and args[4].isdigit():
                target_count = int(args[4])
            count = download_avibase_photos(bird_name, scientific_name, output_dir, avibaseid_override, target_count=target_count)
            if count > 0:
                print(f"âœ… æˆåŠŸä¸‹è½½ {count} å¼ Avibaseç…§ç‰‡")
            else:
                print(f"âš ï¸  æœªèƒ½ä¸‹è½½ç…§ç‰‡")
    except KeyboardInterrupt:
        print("\nâ¹ï¸  å·²ä¸­æ–­")

if __name__ == "__main__":
    main()

